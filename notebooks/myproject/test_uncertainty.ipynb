{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset, ClassLabel, load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    AutoConfig, \n",
    "    PreTrainedTokenizerFast, \n",
    "    GPT2TokenizerFast, \n",
    "    GPT2ForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer import JointTrainer\n",
    "from run_uncertainty import DataCollatorForJointClassification\n",
    "from common_functions import (\n",
    "    entities2dict, \n",
    "    merge_ent_dict, \n",
    "    common_cal, \n",
    "    LockedDropoutMC, \n",
    "    WordDropoutMC, \n",
    "    DropoutMC,\n",
    "    activate_mc_dropout,\n",
    "    convert_dropouts,\n",
    "    convert_to_mc_dropout,\n",
    "    freeze_all_dpp_dropouts,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "train_file = \"../test_data/new_joint_train_NYT_1over4.json\" # 172718\n",
    "validation_file = \"../test_data/new_joint_test_part_NYT.json\" # 1680\n",
    "output_dir = \"../tok_cls_result/NYT_gpt2_logic\"\n",
    "\n",
    "label_all_tokens = False\n",
    "task_name=\"ner\"\n",
    "model_name_or_path = \"../tok_cls_result/NYT_uncertainty_prob_variance/checkpoint-7582/\"  # \"gpt2-medium\"\n",
    "classifier_type = \"crf\"\n",
    "cache_dir = None\n",
    "model_revision = \"main\"\n",
    "use_auth_token = False\n",
    "beta = 1.0\n",
    "alpha = 0.5 \n",
    "boot_start_epoch = 5 \n",
    "threshold = 0.5 \n",
    "use_subtoken_mask = False\n",
    "pad_to_max_length = False\n",
    "fp16 = False\n",
    "padding = \"max_length\" if pad_to_max_length else False\n",
    "preprocessing_num_workers = None\n",
    "overwrite_cache = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=20,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=fp16,\n",
    "    seed=42,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f9a36bd4350fd2bd\n",
      "Found cached dataset json (/home/dsi/yufli/.cache/huggingface/datasets/json/default-f9a36bd4350fd2bd/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 2/2 [00:00<00:00, 202.84it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"json\", data_files={\"train\": train_file, \"validation\": validation_file})\n",
    "column_names = datasets[\"train\"].column_names\n",
    "features = datasets[\"train\"].features\n",
    "text_column_name = \"tokens\" if \"tokens\" in column_names else column_names[0]\n",
    "label_column_name = (f\"{task_name}_tags\" if f\"{task_name}_tags\" in column_names else column_names[1])\n",
    "\n",
    "# In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the unique labels.\n",
    "def get_label_list(labels):\n",
    "    unique_labels = set()\n",
    "    for label in labels:\n",
    "        unique_labels = unique_labels | set(label)\n",
    "    # NOTE Improvements for GPT2+CRF: check if a B-label has its corresponding I-label.\n",
    "    # Related to changes of the label_all_tokens behavior.\n",
    "    ilabels_to_add = set()\n",
    "    if 'O' not in unique_labels:\n",
    "        ilabels_to_add.add('O')\n",
    "    if label_all_tokens:\n",
    "        for ulabel in unique_labels:\n",
    "            if ulabel.startswith(\"B-\"):\n",
    "                ilabel = \"I\" + ulabel[1:] # B-XXX -> I-XXX\n",
    "                if ilabel not in unique_labels:\n",
    "                    ilabels_to_add.add(ilabel)\n",
    "    if ilabels_to_add:\n",
    "        unique_labels = unique_labels | ilabels_to_add\n",
    "        logger.info(f\"Additional labels added: {ilabels_to_add}\")\n",
    "    \n",
    "    label_list = list(unique_labels)\n",
    "    label_list.sort()\n",
    "    return label_list\n",
    "\n",
    "if isinstance(features[label_column_name].feature, ClassLabel):\n",
    "    label_list = features[label_column_name].feature.names\n",
    "    # No need to convert the labels since they are already ints.\n",
    "    label_to_id = {i: i for i in range(len(label_list))}\n",
    "else:\n",
    "    label_list = get_label_list(datasets[\"train\"][label_column_name])\n",
    "    label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "    \n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently download model & vocab.\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id, # Workaround for GPT2 w/o predefined labels\n",
    "    id2label=id2label, # Workaround for GPT2 w/o predefined labels\n",
    "    token_classifier_o_label_id=label2id['O'], # GPT2TokenClassificaton specific\n",
    "    token_classifier_type=classifier_type, # GPT2TokenClassificaton specific\n",
    "    finetuning_task=task_name,\n",
    "    cache_dir=cache_dir,\n",
    "    revision=model_revision,\n",
    "    use_auth_token=True if use_auth_token else None,\n",
    "    beta=beta, # attention loss parameter\n",
    "    alpha=alpha, # logic loss parameter\n",
    "    use_subtoken_mask=use_subtoken_mask,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    revision=model_revision,\n",
    "    use_auth_token=True if use_auth_token else None,\n",
    "    add_prefix_space=True, # Workaround for GPT2\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token # Workaround for GPT2\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=cache_dir,\n",
    "    revision=model_revision,\n",
    "    use_auth_token=True if use_auth_token else None,\n",
    ")\n",
    "# torch.save(self.tokenizer, 'examples/tok_cls_result/tokenizer.pt')\n",
    "# Tokenizer check: this script requires a fast tokenizer.\n",
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    raise ValueError(\n",
    "        \"This example script only works for models that have a fast tokenizer. Checkout the big table of models \"\n",
    "        \"at https://huggingface.co/transformers/index.html#bigtable to find the model types that meet this \"\n",
    "        \"requirement\"\n",
    "    )\n",
    "\n",
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=512, # Workaround for GPU memory consumption\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    labels = []\n",
    "    queryID = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        tokens = tokenized_inputs.tokens(batch_index=i) # subtokens after GPT2 Tokenizer\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i) # [0, 1, 1, 2, 3, 3, 3, 4, ...]\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for j, word_idx in enumerate(word_ids):\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx: # and tokens[j].startswith(\"Ġ\"): # ADDED condition, should be held when add_prefix_space=True\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                # label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)\n",
    "                # NOTE Change behavior of label_all_tokens:\n",
    "                # NOTE The word_ids trick does not always work, e.g., a file path /usr/bin/bash will be split into subparts with different word_ids.\n",
    "                # To solve this problem, as we specific add_prefix_space=True, we can use the leading Ġ to check word boundary.\n",
    "                if label_all_tokens:\n",
    "                    if label[word_idx].startswith(\"B-\"):\n",
    "                        ilb = \"I\" + label[word_idx][1:]\n",
    "                        label_ids.append(label_to_id[ilb])\n",
    "                    else: # label starts with \"I-\" or \"O\": directly add it to label_ids\n",
    "                        label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        # add query id\n",
    "        query_id = examples[\"query_ids\"][i]\n",
    "        try:\n",
    "            queryID.append([word_ids.index(query_id)])\n",
    "        except:\n",
    "            queryID.append([0])\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    tokenized_inputs[\"query_ids\"] = queryID\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 173/174 [00:51<00:00,  3.33ba/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  3.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the dataset\n",
    "# Data collator\n",
    "data_collator = DataCollatorForJointClassification(tokenizer, pad_to_multiple_of=8 if fp16 else None)\n",
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "# Datasets\n",
    "train_dataset = datasets[\"train\"]\n",
    "eval_dataset = datasets[\"validation\"]\n",
    "# Tokenize datasets\n",
    "init_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    num_proc=preprocessing_num_workers,\n",
    "    load_from_cache_file=not overwrite_cache,\n",
    ")\n",
    "\n",
    "\n",
    "def group_sub_entities(entities: List[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Group together the adjacent tokens with the same entity predicted.\n",
    "    Args:\n",
    "        entities (:obj:`dict`): The entities predicted by the pipeline (List of entity dicts).\n",
    "    \"\"\"\n",
    "    # Get the first entity in the entity group\n",
    "    entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "    tokens = [entity[\"word\"] for entity in entities]\n",
    "    index = [entity[\"index\"] for entity in entities]\n",
    "\n",
    "    entity_group = {\n",
    "        \"entity_group\": entity,\n",
    "        \"word\": tokenizer.convert_tokens_to_string(tokens),\n",
    "        \"index\": index,\n",
    "    }\n",
    "    return entity_group\n",
    "\n",
    "\n",
    "def group_entities(ignore_subwords, entities: List[dict]) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Find and group together the adjacent tokens with the same entity predicted.\n",
    "    Args:\n",
    "        entities (:obj:`dict`): The entities predicted by the pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    entity_groups = []\n",
    "    entity_group_disagg = []\n",
    "\n",
    "    if entities:\n",
    "        last_idx = entities[-1][\"index\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        is_last_idx = entity[\"index\"] == last_idx\n",
    "        is_subword = ignore_subwords and entity[\"is_subword\"]\n",
    "        if not entity_group_disagg:\n",
    "            if not is_subword: # the first entity can never be a subword\n",
    "                entity_group_disagg += [entity]\n",
    "            if is_last_idx and entity_group_disagg:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg)]\n",
    "            # print(\"entity group disagg: {}\".format(entity_group_disagg))\n",
    "            continue\n",
    "\n",
    "        # If the current entity is similar and adjacent to the previous entity, append it to the disaggregated entity group\n",
    "        # The split is meant to account for the \"B\" and \"I\" suffixes\n",
    "        # Shouldn't merge if both entities are B-type\n",
    "        if (\n",
    "            (\n",
    "                entity[\"entity\"].split(\"-\")[-1] == entity_group_disagg[-1][\"entity\"].split(\"-\")[-1]\n",
    "                and entity[\"entity\"].split(\"-\")[0] != \"B\"\n",
    "            )\n",
    "            and entity[\"index\"] == entity_group_disagg[-1][\"index\"] + 1\n",
    "        ) or is_subword:\n",
    "            # Modify subword type to be previous_type\n",
    "            if is_subword:\n",
    "                entity[\"entity\"] = entity_group_disagg[-1][\"entity\"].split(\"-\")[-1]\n",
    "                # print(\"entity (after aligning tag): {}\".format(entity))\n",
    "\n",
    "            entity_group_disagg += [entity]\n",
    "            # Group the entities at the last entity\n",
    "            if is_last_idx:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg)]\n",
    "        # If the current entity is different from the previous entity, aggregate the disaggregated entity group\n",
    "        else:\n",
    "            entity_groups += [group_sub_entities(entity_group_disagg)]\n",
    "            entity_group_disagg = [entity]\n",
    "            # If it's the last entity, add it to the entity groups\n",
    "            if is_last_idx:\n",
    "                entity_groups += [group_sub_entities(entity_group_disagg)]\n",
    "\n",
    "    return entity_groups\n",
    "\n",
    "\n",
    "def handling_score(\n",
    "    labels_idx, input_ids, special_tokens_mask, \n",
    "    gen_labels, grouped_entities, ignore_subwords, \n",
    "    apply_gpt2_subword_mask, detect_gpt2_leading_space, \n",
    "    ignore_labels, is_label=False,\n",
    "):  \n",
    "    entities = []\n",
    "    # Filter to labels not in `self.ignore_labels`\n",
    "    # Filter special_tokens\n",
    "    filtered_labels_idx = []\n",
    "    true_idx = None\n",
    "\n",
    "    if is_label: # handling groud truth labels\n",
    "        for idx, label_idx in enumerate(labels_idx):\n",
    "            if label_idx == -100: # subword\n",
    "                if true_idx is not None and idx == true_idx + 1: # the current subword belongs to the latest true token\n",
    "                    filtered_labels_idx.append((idx, label_idx))\n",
    "                    true_idx += 1 # the true idx is updated as the current idx\n",
    "            elif (\n",
    "                model.config.id2label[label_idx] not in ignore_labels \n",
    "                and not special_tokens_mask[idx]\n",
    "            ):\n",
    "                true_idx = idx # record the latest true idx\n",
    "                filtered_labels_idx.append((idx, label_idx))\n",
    "    else: # handling predictions\n",
    "        for idx, label_idx in enumerate(labels_idx):\n",
    "            if gen_labels[idx] == -100: # subword\n",
    "                if true_idx is not None and idx == true_idx + 1: # the current subword belongs to the latest true token\n",
    "                    filtered_labels_idx.append((idx, label_idx))\n",
    "                    true_idx += 1 # the true idx is updated as the current idx\n",
    "            elif (\n",
    "                model.config.id2label[label_idx] not in ignore_labels \n",
    "                and not special_tokens_mask[idx]\n",
    "            ):\n",
    "                true_idx = idx # record the latest true idx\n",
    "                filtered_labels_idx.append((idx, label_idx))\n",
    "\n",
    "\n",
    "    for idx, label_idx in filtered_labels_idx:\n",
    "        word = tokenizer.convert_ids_to_tokens([int(input_ids[idx])])[0] # contains \"Ġ\"\n",
    "        is_subword = False\n",
    "        # NOTE Patch for GPT2 subword detection by using word_ids (ref. line 192)\n",
    "        if apply_gpt2_subword_mask:\n",
    "            is_subword = gen_labels[idx] == -100\n",
    "        \n",
    "        # NOTE GPT2 specific subword detection for special words like IP/Email addresses\n",
    "        # Only be correct when add_prefix_space = True in Tokenizer\n",
    "        if detect_gpt2_leading_space:\n",
    "            is_subword = is_subword or not word.startswith('Ġ')\n",
    "\n",
    "        if int(input_ids[idx]) == tokenizer.unk_token_id:\n",
    "            is_subword = False\n",
    "\n",
    "        if is_subword:\n",
    "            entity = {\n",
    "                \"word\": word,\n",
    "                \"entity\": \"B-X\",\n",
    "                \"index\": idx,\n",
    "            }\n",
    "        else:\n",
    "            entity = {\n",
    "                \"word\": word,\n",
    "                \"entity\": model.config.id2label[label_idx],\n",
    "                \"index\": idx,\n",
    "            }\n",
    "\n",
    "        if grouped_entities and ignore_subwords:\n",
    "            entity[\"is_subword\"] = is_subword\n",
    "\n",
    "        entities += [entity]\n",
    "\n",
    "    if grouped_entities:\n",
    "        return group_entities(ignore_subwords, entities) # Append ungrouped entities\n",
    "    else:\n",
    "        return entities\n",
    "\n",
    "\n",
    "def preds_to_grouped_entity(\n",
    "        preds: Union[np.ndarray, Tuple[np.ndarray]] = None,\n",
    "        is_label: bool = False,\n",
    "        ignore_labels=[\"O\"],\n",
    "        grouped_entities: bool = True,\n",
    "        ignore_subwords: bool = True,  \n",
    "        detect_gpt2_leading_space: bool = False, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    preds (np.ndarray: N X T X T X V): prediction logits from model outputs.\n",
    "    \"\"\"\n",
    "    if detect_gpt2_leading_space:\n",
    "        if not isinstance(tokenizer, GPT2TokenizerFast):\n",
    "            raise ValueError(\"tokenizer must be a GPT2TokenizerFast\")\n",
    "        if not tokenizer.add_prefix_space:\n",
    "            raise ValueError(\"tokenizer.add_prefix_space must be set to True when detect_gpt2_leading_space is True.\")\n",
    "\n",
    "    if ignore_subwords and not tokenizer.is_fast:\n",
    "        raise ValueError(\n",
    "            \"Slow tokenizers cannot ignore subwords. Please set the `ignore_subwords` option\"\n",
    "            \"to `False` or use a fast tokenizer.\"\n",
    "        )\n",
    "\n",
    "    if isinstance(model, GPT2ForTokenClassification) and ignore_subwords:\n",
    "        apply_gpt2_subword_mask = True\n",
    "    else:\n",
    "        apply_gpt2_subword_mask = False\n",
    "\n",
    "    answers = []\n",
    "    all_input_ids = eval_dataset[\"input_ids\"] # N X T\n",
    "    all_special_tokens_mask = eval_dataset[\"special_tokens_mask\"] # N X T\n",
    "    all_labels = eval_dataset[\"labels\"] # N X T\n",
    "\n",
    "    if not is_label: # preds is prediction N X T X T X V\n",
    "        for i, logits in enumerate(preds): # logits: T X T X V (T is after padding)\n",
    "            input_ids = all_input_ids[i] # T (non-padding)\n",
    "            special_tokens_mask = all_special_tokens_mask[i] # T (non-padding)\n",
    "            labels = all_labels[i] # T (non-padding)\n",
    "            logits = logits[:len(input_ids), :len(input_ids)] # remove the padding part of the logit\n",
    "            sent_res = []\n",
    "            \n",
    "            for query, logit in enumerate(logits): # T X V\n",
    "                score = np.exp(logit) / np.exp(logit).sum(-1, keepdims=True) # T X V\n",
    "                labels_idx = score.argmax(axis=-1) # T\n",
    "\n",
    "                seq_entities = handling_score(\n",
    "                    labels_idx, input_ids, special_tokens_mask, \n",
    "                    labels, grouped_entities, ignore_subwords, apply_gpt2_subword_mask, \n",
    "                    detect_gpt2_leading_space, ignore_labels, is_label=False,\n",
    "                ) # List[Dict] 1-D\n",
    "                sent_res.append(seq_entities) # List[List[Dict]] 2-D\n",
    "    \n",
    "            answers.append(sent_res)\n",
    "    \n",
    "    else: # preds are labels N X T\n",
    "        for i, label_ids in enumerate(preds): # label_ids: T (T is after padding)\n",
    "            input_ids = all_input_ids[i] # T (non-padding)\n",
    "            special_tokens_mask = all_special_tokens_mask[i] # T (non-padding)\n",
    "            labels_idx = label_ids[:len(input_ids)] # remove the padding part of the labels\n",
    "            score = np.array([[1.0]*len(model.config.id2label)]*len(labels_idx)) # T X V\n",
    "        \n",
    "            seq_entities = handling_score(\n",
    "                labels_idx, input_ids, \n",
    "                special_tokens_mask, labels_idx,\n",
    "                grouped_entities, ignore_subwords, \n",
    "                apply_gpt2_subword_mask, detect_gpt2_leading_space, \n",
    "                ignore_labels, is_label=True,\n",
    "            ) # List[Dict] 1-D\n",
    "            answers.append(seq_entities)\n",
    "\n",
    "    if len(answers) == 1:\n",
    "        return answers[0]\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def extract_triplets(grouped_entities, dataset_name=\"eval\", is_label=True):\n",
    "    \"\"\"\n",
    "    dataset: self.train_dataset or self.eval_dataset\n",
    "    \"\"\" \n",
    "    if dataset_name == \"eval\":\n",
    "        dataset = eval_dataset\n",
    "    else:\n",
    "        dataset = train_dataset\n",
    "    sentIDs = dataset['sentID']\n",
    "    queryIDs = dataset['query_ids']\n",
    "\n",
    "    if is_label: # extract triplets from grouped_labels (N X T)\n",
    "        label_entities = []\n",
    "        ID_set = set()\n",
    "        for i, entities in enumerate(grouped_entities):\n",
    "            sentid, queryid = sentIDs[i], queryIDs[i][0] # each instance\n",
    "            if sentid not in ID_set: # new sentence\n",
    "                if i != 0: # not the first instance\n",
    "                    merge_ent_dict(ent_dict, sent_ents) # merge all the entities and relations into triplets\n",
    "                    label_entities.append(sent_ents) # append each sentence triplets to output\n",
    "\n",
    "                ID_set.add(sentid)\n",
    "                sent_ents = []\n",
    "                ent_dict = defaultdict(dict)\n",
    "            \n",
    "            entities2dict(entities, queryid, ent_dict) # build entity-relations dict\n",
    "\n",
    "            if i == len(grouped_entities) - 1: # last instance\n",
    "                merge_ent_dict(ent_dict, sent_ents) # merge all the entities and relations into triplets\n",
    "                label_entities.append(sent_ents) # append each sentence triplets to output\n",
    "\n",
    "    else: # extract triplets from grouped_preds (N X T X T')\n",
    "        label_entities = []\n",
    "        all_labels = dataset['labels']\n",
    "        all_sentIDs = dataset['sentID']\n",
    "        unique_pair = []\n",
    "        id_set = set()\n",
    "        for Id, tag in zip(all_sentIDs, all_labels):\n",
    "            if Id not in id_set:\n",
    "                id_set.add(Id)\n",
    "                unique_pair.append((Id, tag))\n",
    "\n",
    "        for i, sentence_entities in enumerate(grouped_entities): # every sentence (T X T')\n",
    "            sent_ents = []\n",
    "            label = unique_pair[i][1] # corresponding labels\n",
    "            ent_dict = defaultdict(dict) # record each entities and related entities for each sentence\n",
    "            for queryid, entities in enumerate(sentence_entities): # every query instance (T')\n",
    "                if label[queryid] != -100: # we only extract triplets for non-subword positions\n",
    "                    entities2dict(entities, queryid, ent_dict) # build entity-relations dict  \n",
    "\n",
    "            merge_ent_dict(ent_dict, sent_ents) # merge all the entities and relations into triplets\n",
    "            label_entities.append(sent_ents)\n",
    "\n",
    "    return label_entities\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    predictions logits (np.ndarray: N X T X T X V): # instances X query dimension X token dimension X label dimension.\n",
    "    labels (np.ndarray: N X T (Dict)): ground truth of label_ids (corresponding to query_ids).\n",
    "    \"\"\"\n",
    "    predictions, labels = p # N X T X T X V, N X T\n",
    "    grouped_preds = preds_to_grouped_entity(preds=predictions)\n",
    "    # remove repeated preds for the same sentence\n",
    "    sent_id_pool = set()\n",
    "    remove_idx = []\n",
    "    for i, sent_id in enumerate(eval_dataset[\"sentID\"]):\n",
    "        if sent_id not in sent_id_pool:\n",
    "            sent_id_pool.add(sent_id)\n",
    "        else:\n",
    "            remove_idx.append(i)\n",
    "    \n",
    "    grouped_preds = [preds for i, preds in enumerate(grouped_preds) if i not in remove_idx] # N' X T X T'\n",
    "    grouped_labels = preds_to_grouped_entity(preds=labels, is_label=True) # N X T'\n",
    "    \n",
    "    true_predictions = extract_triplets(grouped_preds, is_label=False) # N X T' (quadratic dict)\n",
    "    true_labels = extract_triplets(grouped_labels, is_label=True) # N X T' (quadratic dict)\n",
    "\n",
    "    with open(\"pred_triplets(gpt2).csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(true_predictions)\n",
    "\n",
    "    with open(\"label_triplets(gpt2).csv\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(true_labels)       \n",
    "\n",
    "    TP_notag, TP_tag, Pos, Neg = 0, 0, 0, 0\n",
    "    pred_F, ent_mention_F, ent_tag_F = 0, 0, 0\n",
    "    re_mention_F, re_tag_FN, re_tag_FP, re_tag_F = 0, 0, 0, 0\n",
    "    # calculate precision, recall, F1 and accuracy\n",
    "    for hyp, ref in zip(true_predictions, true_labels):\n",
    "        tp_notag, tp_tag, n_hyp, n_ref, false_tag, ent_mention_f, ent_tag_f, \\\n",
    "            re_mention_f, re_fn, re_fp, re_tag_f = common_cal(hyp, ref)\n",
    "        TP_notag += tp_notag\n",
    "        TP_tag += tp_tag\n",
    "        Pos += n_hyp\n",
    "        Neg += n_ref\n",
    "        pred_F += false_tag\n",
    "        ent_mention_F += ent_mention_f\n",
    "        ent_tag_F += ent_tag_f\n",
    "        re_mention_F += re_mention_f\n",
    "        re_tag_FN += re_fn\n",
    "        re_tag_FP += re_fp\n",
    "        re_tag_F += re_tag_f\n",
    "\n",
    "    pre_notag = TP_notag / Pos if Pos else 0.0\n",
    "    rec_notag = TP_notag / Neg if Neg else 0.0\n",
    "    f1_notag = 2.0 * pre_notag * rec_notag / (pre_notag + rec_notag) if (pre_notag or rec_notag) else 0.0\n",
    "\n",
    "    pre_tag = TP_tag / Pos if Pos else 0.0\n",
    "    rec_tag = TP_tag / Neg if Neg else 0.0\n",
    "    f1_tag = 2.0 * pre_tag * rec_tag / (pre_tag + rec_tag) if (pre_tag or rec_tag) else 0.0\n",
    "\n",
    "    ent_m_fr = ent_mention_F / pred_F if pred_F else 0.0\n",
    "    ent_tag_fr = ent_tag_F / pred_F if pred_F else 0.0\n",
    "    re_m_fr = re_mention_F / pred_F if pred_F else 0.0\n",
    "    re_tag_fnr = re_tag_FN / pred_F if pred_F else 0.0\n",
    "    re_tag_fpr = re_tag_FP / pred_F if pred_F else 0.0\n",
    "    re_tag_fr = re_tag_F / pred_F if pred_F else 0.0\n",
    "    \n",
    "    pred_len = [len(pred) for pred in true_predictions]\n",
    "    avg_pred_len = sum(pred_len) / len(pred_len) if len(pred_len) else 0.0\n",
    "    label_len = [len(label) for label in true_labels]\n",
    "    avg_label_len = sum(label_len) / len(label_len) if len(label_len) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": pre_notag,\n",
    "        \"recall\": rec_notag,\n",
    "        \"f1\": f1_notag, \n",
    "        \"precision(tag)\": pre_tag,\n",
    "        \"recall(tag)\": rec_tag,\n",
    "        \"f1(tag)\": f1_tag,\n",
    "        \"ent_mention_fr\": ent_m_fr,\n",
    "        \"ent_tag_fr\": ent_tag_fr,\n",
    "        \"re_mention_fr\": re_m_fr,\n",
    "        \"re_fpr\": re_tag_fpr,\n",
    "        \"re_fnr\": re_tag_fnr,\n",
    "        \"re_tag_fr\": re_tag_fr,\n",
    "        \"avg_pred_len\": avg_pred_len,\n",
    "        \"avg_true_len\": avg_label_len,\n",
    "    }\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = JointTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=init_dataset, # we feed the intial training data to the Trainer\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    bootstrap=None,\n",
    "    original_dataset=train_dataset,\n",
    "    use_bootstrap=False,\n",
    "    boot_start_epoch=boot_start_epoch,\n",
    ")\n",
    "# Dataloaders\n",
    "selected_dataset = init_dataset.select(range(1000))\n",
    "# train_dataloader = trainer.get_train_dataloader(selected_dataset) # 5\n",
    "train_dataloader = DataLoader(\n",
    "            selected_dataset, \n",
    "            batch_size=20,\n",
    "            shuffle=False, \n",
    "            collate_fn=data_collator,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "eval_dataloader = trainer.get_eval_dataloader(eval_dataset)\n",
    "train_batch0 = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:55,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from common_functions import bald, probability_variance, sampled_max_prob\n",
    "threshold = 0.5\n",
    "committee_size = 15\n",
    "\n",
    "dropout = Namespace(\n",
    "    max_n=100,\n",
    "    max_frac=0.4,\n",
    "    mask_name='mc',\n",
    "    dry_run_dataset='train',\n",
    ")\n",
    "args = Namespace(\n",
    "    dropout_type='MC',\n",
    "    inference_prob=0.1,\n",
    "    committee_size=committee_size, # number of forward passes\n",
    "    dropout_subs='last',\n",
    "    eval_bs=1000,\n",
    "    use_cache=True,\n",
    "    eval_passes=False,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "# # Inference (no dropout)\n",
    "# eval_results = {}\n",
    "# eval_results[\"sampled_probabilities\"] = []\n",
    "# logger.info(\"****************Start runs**************\")\n",
    "\n",
    "# for i in tqdm(range(args.committee_size)):\n",
    "#     outputs = model(**train_batch0) # loss, logits, position_attentions\n",
    "#     probs = F.softmax(outputs.logits.float(), dim=-1) # B X T X C\n",
    "#     preds = torch.argmax(probs, dim=-1) # B X T\n",
    "#     eval_results[\"sampled_probabilities\"].append(probs.tolist())\n",
    "# logger.info(\"Done!!!\")\n",
    "\n",
    "# Stochastic inference： MC Dropout\n",
    "logger.info(\"*** Evaluate ***\")\n",
    "logger.info(\"******Perform stochastic inference...*******\")\n",
    "convert_dropouts(model, args)\n",
    "activate_mc_dropout(model, activate=True, random=args.inference_prob)\n",
    "logger.info(\"****************Start runs**************\")\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "model.eval()\n",
    "\n",
    "matched_idx1, matched_idx2, matched_idx3 = [], [], []\n",
    "mscores1, mscores2, mscores3 = [], [], []\n",
    "\n",
    "for step, inputs in tqdm(enumerate(train_dataloader)):\n",
    "    # MC Dropout: stochastic inference\n",
    "    dropout_eval_results = {}\n",
    "    dropout_eval_results[\"sampled_probabilities\"] = []\n",
    "    # Prepare inputs {\"input_ids\", \"query_ids\", \"target_att\", ...} \n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(args.committee_size):\n",
    "            outputs = model(**inputs) # loss, logits, position_attentions\n",
    "            probs = F.softmax(outputs.logits.float(), dim=-1) # B X T X C\n",
    "            dropout_eval_results[\"sampled_probabilities\"].append(probs.tolist()) # K X B X T X C\n",
    "\n",
    "    prob_array = np.array(dropout_eval_results[\"sampled_probabilities\"]) # K X B X T X C\n",
    "\n",
    "    # Uncertainty estimation\n",
    "    s1 = bald(prob_array) # B\n",
    "    s2 = sampled_max_prob(prob_array) # B\n",
    "    s3 = probability_variance(prob_array) # B\n",
    "\n",
    "    mscores1.extend(s1.tolist())\n",
    "    mscores2.extend(s2.tolist())\n",
    "    mscores3.extend(s3.tolist())\n",
    "    \n",
    "    # soft_thre1 = sorted(s1)[int(0.8*len(s1))]\n",
    "    batch_idx1 = (np.where(s1 < threshold)[0] + step * 20)\n",
    "    matched_idx1.extend(batch_idx1.tolist())\n",
    "\n",
    "    # soft_thre2 = sorted(s2)[int(0.8*len(s2))]\n",
    "    batch_idx2 = (np.where(s2 < threshold)[0] + step * 20)\n",
    "    matched_idx2.extend(batch_idx2.tolist())\n",
    "\n",
    "    # soft_thre3 = sorted(s3)[int(0.8*len(s3))]\n",
    "    batch_idx3 = (np.where(s3 <= threshold)[0] + step * 20)\n",
    "    matched_idx3.extend(batch_idx3.tolist())\n",
    "\n",
    "activate_mc_dropout(model, activate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mscores1)\n",
    "# print(mscores2)\n",
    "# print(mscores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 132, 133, 135, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 159, 160, 161, 162, 163, 165, 166, 168, 169, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 194, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 216, 217, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 281, 284, 285, 286, 287, 289, 290, 293, 294, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 311, 312, 313, 314, 315, 317, 318, 320, 321, 322, 323, 324, 325, 326, 327, 328, 331, 332, 333, 334, 335, 336, 337, 338, 340, 342, 343, 344, 350, 356, 357, 358, 360, 361, 362, 364, 366, 368, 369, 370, 371, 372, 373, 374, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 400, 401, 402, 405, 406, 409, 410, 411, 414, 415, 416, 417, 418, 419, 420, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 437, 438, 439, 440, 441, 445, 446, 448, 449, 453, 454, 455, 456, 458, 459, 460, 461, 464, 465, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 479, 481, 482, 484, 485, 486, 488, 492, 493, 495, 496, 497, 498, 499, 500, 501, 502, 504, 505, 506, 507, 508, 509, 510, 511, 513, 514, 515, 516, 518, 519, 520, 521, 522, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 542, 544, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 559, 560, 561, 562, 563, 565, 566, 567, 568, 569, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 603, 604, 605, 607, 608, 611, 612, 613, 614, 615, 616, 617, 618, 619, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 634, 635, 636, 637, 642, 643, 644, 645, 646, 647, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 688, 689, 692, 693, 694, 695, 696, 697, 698, 700, 701, 703, 704, 705, 706, 707, 708, 709, 711, 712, 716, 717, 718, 719, 720, 721, 722, 723, 724, 727, 729, 730, 731, 733, 734, 735, 736, 737, 739, 741, 742, 743, 744, 746, 747, 748, 749, 750, 753, 754, 755, 756, 757, 758, 759, 761, 762, 764, 765, 766, 767, 768, 769, 771, 772, 773, 774, 775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 790, 791, 792, 793, 794, 795, 796, 798, 800, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 822, 823, 824, 826, 827, 829, 830, 832, 833, 836, 837, 838, 839, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 855, 856, 857, 858, 860, 861, 862, 863, 864, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 885, 886, 887, 889, 890, 891, 893, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 910, 911, 912, 913, 914, 915, 916, 918, 919, 920, 922, 924, 925, 927, 928, 929, 930, 931, 933, 934, 935, 936, 937, 938, 939, 940, 942, 944, 945, 946, 947, 948, 949, 950, 954, 955, 957, 958, 959, 960, 962, 963, 964, 965, 966, 967, 968, 969, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 992, 993, 994, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "# print(len(matched_idx1), matched_idx1)\n",
    "# print(len(matched_idx2), matched_idx2)\n",
    "# print(len(matched_idx3), matched_idx3)\n",
    "model_thre = sorted(mscores1)[int(0.7*len(mscores1))]\n",
    "matched_idx = (np.where(np.array(mscores1) <= model_thre)[0]).tolist()\n",
    "print(len(matched_idx), matched_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # stop dropout\n",
    "model(**train_batch0).position_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.normal(0, 1, 10)\n",
    "print(test)\n",
    "np.where(test > 0.5)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_uncertainty(preds, ue='vanilla'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        preds: B X T X C\n",
    "    Output:\n",
    "        scores: B\n",
    "    \"\"\"\n",
    "    if ue == 'vanilla':\n",
    "        token_score = torch.max(preds, dim=-1)[0] # B X T\n",
    "    elif ue == 'entropy':\n",
    "        token_score = torch.sum(-preds * torch.log(torch.clip(preds, 1e-8, 1)), axis=-1) # B X T\n",
    "    else:\n",
    "        raise ValueError('Unknown uncertainty estimation method.')\n",
    "    score = torch.mean(token_score, dim=-1) # B\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "        0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
       "        0.9992, 0.9992], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_uncertainty(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddd82facefbabeaac52e36426c845dcb25ba5dd6055e7ccfbcee0fd752d3db55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
